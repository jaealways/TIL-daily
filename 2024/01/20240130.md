# TOC
- [PAPER](#paper)
  * [Diffusion Recommendation](#diffusion-recommendation)
- [CS](#cs)
  * [PS](#ps)
    + [150. Evaluate Reverse Polish Notation](#150-evaluate-reverse-polish-notation)


# Goal

- [x] PS: 150. Evaluate Reverse Polish Notation
- [x] PAPER: When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories	
- [ ] LEC: FRM lec 1-1,2



# PAPER
## Diffusion Recommendation

- Encoder, Decoder 사이의 값을 최소화하는 방식으로 학습
- 비전은 이미지 복원하고, 다시 latent 압축하는 것인데 추천에선 무엇?
  - 모델이 구매했다고 예측했는데, 실제 구매하지 않은 것 등을 loss로 할 수 있음
  - User 별로 하든 장르별로 하든 latent encoder에 넣음.
  - 60000개를 사용할 때보다 적은 리소스?
  - reconstruction과 denoising의 경중을 따지기 애매함.
    - 어떤게 중요한지에 따라 둘의 비중(?)  
- 엄청나게 많은 메모리의 감소: Diffusion?
- 실제 노이즈를 주입했을 때, 어떤가

# CS
## PS
### 150. Evaluate Reverse Polish Notation
- pointer 기반으로 이동할 때, 숫자 하나만 있는 경우를 생각하지 못함.
- 항상 length 하나이거나 0일 때도 생각하기

