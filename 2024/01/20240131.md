# TOC
- [PAPER](#paper)
  * [GPT4Rec](#gpt4rec)
    + [Methodologies](#methodologies)
  * [Two-way Multi-Label Loss](#two-way-multi-label-loss)
  * [Improving fine-grained understanding in image-text pre-training](#improving-fine-grained-understanding-in-image-text-pre-training)
  * [Company Day: TILDA](#company-day--tilda)


# Goal

- [x] PS: 150. Evaluate Reverse Polish Notation
- [x] PAPER: Can Large Language Models Be an Alternative to Human Evaluation?	
- [ ] LEC: FRM lec 1-1,2



# PAPER
## GPT4Rec

- NLP 기반 모델들을 활용해서 추천에 사용함
- Discriminative model 대산 gpt 모델을 활용해서 추천 시스템에 활용
- multi-query beam search를 활용해서 multi-interest 활용
- 쿼리로 검색하면 cold start 문제를 해결할 수 있다
- item inventory도 해결 가능?

### Methodologies
- 언어 공간에서 유저의 interest를 생성
- 프롬프트를 사용해서 생성
- GPT2를 파인튜닝함
- 쿼리가 많아질수록 beauty와 electronics 아마존 데이터 커짐.
- Search engine으로 아이템의 공간을 찾음, 어

## Two-way Multi-Label Loss
- 기존 multi-label에 사용하던 loss를 softmax를 일반화시켜서 나타냄
- 클래스 개수만큼 binary cross entropy 계산
  - 그림에서 타겟인 기린을 찾을 때, negative 영역이 훨씬 많음
  - binary imbalance issue
- hard negative: positive와 가까운 항, multilabel 적용시 positive로 분리될 수도 있기 때문
  - 어떻게 하면, hard negative 거리 떨어뜨릴 수 있을까

## Improving fine-grained understanding in image-text pre-training
- Fine-grained contrasive alignment

## Company Day: TILDA
- 회사는 AI를 막연하게 여김, 시장엔 적합한 AI 없음
- 과거 데이터를 기반으로 미래를 예측했었음
- 예측 뿐만 아니라, 원치 않는 상황에 적절한 대응할 수 있어야 됨
  - Prescriptive: 예측 + 처방
- 결정이 시스테믹하기 때문에 사람마다, 혹은 같은 사람이어도 결정 달라짐
