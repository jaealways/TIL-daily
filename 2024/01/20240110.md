# TOC
- [Paste Your Document In Here](#paste-your-document-in-here)
  * [And a table of contents](#and-a-table-of-contents)
  * [On the right](#on-the-right)

- https://ecotrust-canada.github.io/markdown-toc/


# Goal

- [ ] PS: 
- [ ] PAPER: Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents
- [ ] DEV: Entity mask, type entity masker
- [ ] LEC: KLUE
- [ ] STUDY: Github Actions


# DEV

## Pytorch
- tensor 생성시 device 지정 가능, cpu, gpu를 각각 tensor로 연산하면 에러
- extremely small epsilon to prevent inf
    - 1e-7 보정값


# PAPER

## Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss
- 


## Going Beyond Local; Global-Graph-Enhanced Personalized News Recommendation
[Going Beyond Local; Global-Graph-Enhanced Personalized News Recommendation](https://arxiv.org/pdf/2307.06576.pdf)
- 뉴스 추천 논문
- 뉴스 추천은 content based로 해왔음
- 최근엔 유저의 reading history를 시퀀스로 인식해서 모델링
- 또한 그래프 기반의 방법들 제시
- 그러나 유저 한 명의 관점만 반영, global representation 반영해보자
- global news graph를 잘 생성하자
-

### SOLUTION: GLORY
- global news graph는 히스토리에 대한 글로벌 뉴스 제안
- 위에서 뽑은 candidate news (dc) 가져와서 global entity graph 활용해서 둘의 유사도 이루어짐
- historical news에서 Multi-head self attention 사용

### Method
- dc라는 뉴스를 유저가 얼마나 좋아할지 예측하기
- Local representation
- global news graph -> 방향 있는 간선으로

### additional
- cold start
- transductive model vs inductive model
    - inductive: 전통적인 지도학습이랑 같음. 분류 혹은 회귀를 위한 규칙 추론
    - transductive: 

## Visual Programming: Compositional visual reasoning without training

- neuro 기반 인공지능: 예외적인 문제 해결 힘듦 
    - 가령 빨간 불을 불타는 신호등으로 인식
- LLM의 인간의 추론 능력을 가져옴, VISPROG
- VISPROG: cv 문제 해결허기 위해 LLM 활용
    - Instructions을 LLM이 이해해서 코드로 변환

- 성능 테스트를 위해 아래 네 가지 태스크 진행
    - compositional visual question answering
    - Zero-Shot Reasoning on Image Pairs
    - Factual Knowledge Object Tagging
    - Image Editing with Natural Language
- 보통 end-to-end 문제를 해결하는 방식으로 진행
- 롱테일 (소수의 예외적인) 문제를 해결하긴 힘듦
- 상황에 맞는 예제를 제공하면 
- 맞는 모듈을 어떻게 select? -> 사용자 지정

## Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation

- 추천에선 전이학습의 한계 (아마존 고객과 네이버 고객 특성 다름)
- 사용자는 자연어처럼 토큰화하기 어려움

[CLUE 네이버 여러 도메인](https://arxiv.org/pdf/2111.11294.pdf)

- cross-domain(CDSR): 아마존 여러 도메인(책, 옷) 구매 정보 서로 융합
- 각각의 도메인마다 선호하는 특성 다른데, 서로 다른 정보 전달하면 negative transfer 줄 수 있음
- 어떻게 negative 대신 positive로 불러올 수 있을까?

- coarse level vs fine level
- 코스는 가장 큰 범위(가령 책) fine은 자세한(가령 로맨스)
- cooperative game theory와 hierarchal contrastive learning 활용함


- 1. negative transfer 줄이자
- 2. sequence 정보도 고려하자
- t 시점까지의 정보를 갖고, t+1을 예측
- 유저 한 명이 d짜리 차원 가짐
- 식3, multi-head attention 갯수만큼 나눔
- cooperative game: 게임에서 승리했을 때, 컨트리뷰션에 따라 보상 다르게 해줌
- shapley value: 누적 보상

