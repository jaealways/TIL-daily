# TOC
- [PAPER - PROMPT](#paper---prompt)
  * [Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts (CHI’23 비전문가는 프롬프트 사용 어려움)](#why-johnny-can-t-prompt--how-non-ai-experts-try--and-fail--to-design-llm-prompts--chi-23-------------------)
    + [TLDR](#tldr)
    + [Limitations](#limitations)
  * [Large Language Models Can Be Easily Distracted by Irrelevant Context (ICML’23, 프롬프트 중간에 이상한 정보 섞기)](#large-language-models-can-be-easily-distracted-by-irrelevant-context--icml-23---------------------)
    + [TLDR](#tldr-1)
    + [Ideation](#ideation)
- [1. prompt template](#1-prompt-template)
  * [****Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models**** (IEEE’22)](#----interactive-and-visual-prompt-engineering-for-ad-hoc-task-adaptation-with-large-language-models------ieee-22-)
    + [TLDR](#tldr-2)
    + [Limitations](#limitations-1)
  * [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](#a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgpt)
    + [TLDR](#tldr-3)
    + [Ideation](#ideation-1)
- [2. prompt application](#2-prompt-application)
  * [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](#autoprompt--eliciting-knowledge-from-language-models-with-automatically-generated-prompts)
    + [TLDR](#tldr-4)
    + [Limitation](#limitation)
  * [Prompting for Multimodal Hateful Meme Classification](#prompting-for-multimodal-hateful-meme-classification)
    + [TLDR](#tldr-5)
    + [Limitation](#limitation-1)
  * [Legal Prompt Engineering for Multilingual Legal Judgement Prediction](#legal-prompt-engineering-for-multilingual-legal-judgement-prediction)
    + [TLDR](#tldr-6)
    + [Limitation](#limitation-2)
  * [Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language](#conversing-with-copilot--exploring-prompt-engineering-for-solving-cs1-problems-using-natural-language)
    + [TLDR](#tldr-7)
    + [Limitation](#limitation-3)
    + [Ideation](#ideation-2)
  * [Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](#rephrase-and-respond--let-large-language-models-ask-better-questions-for-themselves)
    + [TLDR](#tldr-8)



# Goal

- [x] PS: 2225. Find Players With Zero or One Losses
- [ ] PAPER: DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models
- [x] PAPER: Research Prompt papers


# PAPER - PROMPT

- LLM이 비약적으로 발전하면서, 프롬프트를 활용하는 니즈가 커짐
- 반면, 프롬프팅을 통한 output 성능 이슈로 인해, 라이브하게 사용하는 것에 대한 고민 많음
- 특히 하나의 프롬프트가 꾸준하게 잘 동작하는게 중요한데, 이를 위해선 많은 시행착오를 거쳐야 함

## Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts (CHI’23 비전문가는 프롬프트 사용 어려움)

[Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts](https://dl.acm.org/doi/pdf/10.1145/3544548.3581388)

### TLDR

- 비 AI 전문가가 프롬프트 만드는 것은 굉장히 어려움
- LLM 프로토타입인 Design Probe 사용해서 프롬프팅 성공할 수 있는지 관찰함
- 결론은 비전문가들은 체계적으로 디자인하지 못했고, 특히 사람 간의 지시에 따른 경험과 과도한 일반화 경향이 문제

### Limitations

- 참가자들은 제한된 경험으로 과도한 일반화, 인간 간의 상호작용을 통해 시스템을 보는 경향
- 이로 인해 비전문가들이 LLM을 효과적으로 사용하는데는 어려움 있음

## Large Language Models Can Be Easily Distracted by Irrelevant Context (ICML’23, 프롬프트 중간에 이상한 정보 섞기)

[Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/pdf/2302.00093.pdf)

[GitHub - google-research-datasets/GSM-IC: Grade-School Math with Irrelevant Context (GSM-IC) benchmark is an arithmetic reasoning dataset built upon GSM8K, by adding irrelevant sentences in problem descriptions. GSM-IC is constructed to evaluate the distractibility of language models.](https://github.com/google-research-datasets/GSM-IC)

### TLDR

- 여러 프롬프트 기술로 LLM이 얼마나 산만한지 연구하기 위해, 저자들은 아래 데이터셋을 만듦
- GSM-IC 데이터셋 (Study Methodology Section): 문제 설명과 관련 없는 정보를 초등학교 수학문제에 포함시킴
- 관련 없는 정보 포함하면 성능 극적으로 감소


- 위 그림처럼 중간에 Irrelevant Sentence 넣어줌

### Ideation

- Irrelevant Sentence를 넣어주는 위치는 관련 없을까?

# 1. prompt template

## ****Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models**** (IEEE’22)

[PromptIDE: Interactive and Visual Prompt Engineering](https://prompt.vizhub.ai/#)

- Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models

[Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models](https://arxiv.org/pdf/2208.07852.pdf)

[PromptIDE](https://x.ai/prompt-ide/)

- 같은 제품 라인업인지는 모르겠음

## A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT

### TLDR

- LLM과의 효과적인 상호작용을 위한 프롬프트 엔지니어링이 더 중요해짐
- 본 논문은 특정 규칙과 프로세스를 자동화해서 LLM 기능을 향상하고자 함
- 프롬프트 패턴: 도메인 독립적인 패턴을 중심으로 LLM과 작업할 때 출력 및 상호작용을 체계화하고자 함. (Software Pattern 기반으로)


### Ideation

- 몇 가지 패턴의 성능 계선을 이룬다면? → 성능은 어떻게 측정?
- 더 추가적인 패턴 없을까?

# 2. prompt application

## AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts

[AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/pdf/2010.15980.pdf)

### TLDR

- 추가적인 파인튜닝 없이 사전 훈련된 언어모델에서 지식 끌어냄
- AutoPrompt: gradient-guided search를 통해 프롬프트 자동으로 생성하도록 설계된 시스템
- AutoPrompt를 사용하면 수동으로 생성된 프롬프트에 비해 MLM에서 사실을 이끌어내는데 더 높은 정확도 보임
- 수동 프롬프트의 한계와 자동화된 방법의 advantage에 대해 논의함
- gradient-guided search: ??
- Praking vs Fine-tuning: 데이터가 적을 경우 모델 체크포인트를 모두 저장해야 하는 Fine-tuning과 달리, 프롬프트만 저장하면 되는 praking이 효율적


### Limitation

- 레이블이 지정된 데이터에만 의존, 수동 프롬프트의 해석 가능성은 부족
- 불균형한 훈련 데이터로 어려움을 겪는 경우 많음

## Prompting for Multimodal Hateful Meme Classification

[](https://arxiv.org/pdf/2302.04156.pdf)

### TLDR

- PromptHate: RoBERTa를 사용한 프롬프트 기반 모델, 혐오 밈을 분류하고자 함
- 밈 분류를 MLM 문제로 변환. PLM의 내포된 지식을 통해 특정 프롬프트를 설계함
- 프롬프트를 정확히 어떻게 썼다는거??

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/310de290-c4f3-428e-badf-f06a99ae4455/3b91434b-a9c1-4e71-ab93-3141a3c35c3d/Untitled.png)

### Limitation

- 명시적으로 드러나는 Hate meme은 식별 가능하지만, 미묘한 것은 불가능

## Legal Prompt Engineering for Multilingual Legal Judgement Prediction

[Legal Prompt Engineering for Multilingual Legal Judgement Prediction](https://arxiv.org/pdf/2212.02199.pdf)

### TLDR

- EU와 스위스 법원은 다국어 텍스트를 사용해서 법적 프롬프팅(LPE) 사용함
- LPE의 성능은 베이스라인보단 뛰어나지만 SOTA supervised 보단 부족함
- 법에선 긴 문서에 대한 다단계 추론 이슈가 있는데, 현재 법률 프롬프트는 짧은 추론으로 제한됨
- LJP: 사례 문서를 기반으로 법원 결과를 자동으로 예측
- 결론적으로 LPE로 LJP 과제 해결 가능하다


### Limitation

- 법률 주제 전문가와 협력해서 더 나은 법률 프롬프트를 개발할 필요성
- 공공 및 학술 데이터만이 활용됨

## Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language

[Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language](https://arxiv.org/pdf/2210.15157.pdf)

### TLDR

- 자연어 설명을 통해 소스코드 생성하기 위해 깃허브 코파일럿에 대해 의논
- 166개의 프로그래밍 문제에 대한 코파일럿의 성능을 평가해서 첫 시도에서 약 절반을 해결
- 초반에 풀지 못한 문제 중 60% 이상을 문제 설명을 수정한 후에 해결함
- 프롬프트 엔지니어링이 실제 copilot이 해결책 도출하는데 도움 됨


### Limitation

- 파이썬 문제에 초점 맞추었으며, 객체 지향, functional 등은 없음
- 코파일럿과 초보 프로그래머의 상호작용을 관찰하면 프로그래밍 교육에 좋은 영향 미칠 듯

### Ideation

- LLM과 그것을 다루는 특정 집단의 움직임 관찰 → 집단의 성장에 유의미한 방법?

## Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves

[Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves](https://arxiv.org/pdf/2311.04205.pdf)

### TLDR

- LLM이 어떻게 인간의 질문을 응답하는 성능 향상시킬까?
- Rephrase and Respond(RaR): LLM이 응답을 제공하기 전에 인간이 제기한 질문을 다시 rephrase하고 expand하는

