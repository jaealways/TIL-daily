# TOC
- [Paste Your Document In Here](#paste-your-document-in-here)
  * [And a table of contents](#and-a-table-of-contents)
  * [On the right](#on-the-right)

- https://ecotrust-canada.github.io/markdown-toc/


# Goal

- [ ] PS: 
- [ ] PAPER:
- [ ] DEV: pytorch lightning loss function by task
- [ ] DEV: make module of LDAM loss, 
- [ ] DEV: make module of converting pdf to text


# DEV


## error
### cuda error
```shell
Variable._execution_engine.run_backward(
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```
- GPU에 메모리 할당 실패하는 에러
- 해볼 것들
    - 배치 사이즈 줄이기
    - GPU compatibility 체크하기

### use pretrain model

```shell
Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

```

- sequence classification 등의 문제를 풀기 위해 pre-train 된 모델을 받을 때 생김
- klue/roberta-large를 다른 태스크를 해결하는데 사용
- 이 때 새롭게 초기화된 데이터셋이 필요


# PAPER

## Goal-Oriented Multi-Modal Interactive Recommendation with Verbal and Non-Verbal Relevance Feedback

- 멀티모달 interative 추천은 추천 리스트로 언어적, 비언어적 피드백 제공
- 유저가 추천 리스트에 대한 언어적, 비언어적 피드백을 제공해서 학습
- 시스템이 나중에 강화학습을 통해 원하는 정보를 가져오게 함

### 기존 연구의 한계
- BERT와 같은 end to end method는 unstable한 경우가 많음
- 멀티모달 피쳐 action, feedback 등을 단순히 caoncat하는 방식으로 합침
- sparse한 리워드를 사용해서 ??
- GOMMR이라는 새로운 모델 제시
    - Goal Oriented Reinforcement Learning

### GO-POMDP
- action과 거리, 추천아이템과 유사도에 의해 결정됨?
- partilally observable
- 피드백: 타켓 아이템과 현재 아이템의 차이 서술됨

### GOMMIR
- action과 observation 사이
- CLIP: 각 모달리티에 대해 같은 dimensionality 가짐
- TIRG: text image residual gating
- user의 취향을 계속해서 구체화시킴
- post-filter를 사용해서 제거

### policy-based method
- 최적화는 유클리드 거리를 기준으로 
- log thai theta
- negative sampling learning의 영향력을 감소시켜줌


## SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling
- LLM을 scaling하는 방법 중 하나: Depth up-scaling
- upstage ai에서 발표
### LLM scaling
- openai에서 발표. 모델의 성능이 파라미터 수, 데이터 크기, 연산능력, 손실 네가지로 영향
- 모델이 커짐에 따라 성능이 올라감
- 구글에서 발표, 모델 크기 키우면 width와 depth의 영향 커짐
- 깊이 증가, 폭 증가, 전문가 혼합, 지속적인 사전 훈련
- 모델 레이어 수 늘리는 방법
### SOLAR 10.7B
- DUS = Depth scaling + Continued pre-training
- 사전 모델 학습을 통해 기존 모델 레이어 위해 쌓음
- 하드웨어의 한계가 줄었다

### Depthwise scaling
- 원본 모델에서 최종 m개, 복제본 초기 m개 레이어 제거


## ANYTEXT: MULTILINGUAL VISUAL TEXT GENERA-TION AND EDITING
- 이미지 합성 기술 많이 발전했음
- OCR 기술은 훌륭하지만, 생성은 어색함
- 멀티 라인, 영역이 정확하지 않더라도(deformed region) 등도 잘됨
- plug and play: 기존 디퓨전 모델과도 호환 잘됨
- 시각적 텍스트 생성하는데 어려움
    - 기존 대규모 이미지데이터 셋은 텍스트 관련 데이터 없음
    - CLIP: 어휘 토크나이저에선 큰 문제로 작용하지 않음 -> 텍스트 생성에 문제
    - 손실함수에 대한 방법론 없음
- 글리프 라인을 이미지로 랜더링하고 정보를 인코딩해서 다시 OCR? -> 시각적 특성까지 뽑아낼 수 있음

### Text control diffusion pipeline
- VAE는 생성 모델의 일종
- 노이즈를 추가한 것과의 차이를 minimize하도록 loss 설계

