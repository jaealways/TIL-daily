# TOC
- [CS](#cs)
  * [PS](#ps)
    + [645. Set Mismatch](#645-set-mismatch)
- [PAPER](#paper)
  * [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](#llama-vid--an-image-is-worth-2-tokens-in-large-language-models)
  * [Generative Agents: Interactive Simulacra of Human Behavior](#generative-agents--interactive-simulacra-of-human-behavior)
  * [Osprey: Pixel Understanding with Visual Instruction Tuning](#osprey--pixel-understanding-with-visual-instruction-tuning)


# Goal

- [x] PS: 645. Set Mismatch
- [x] PAPER: ProQA: Structural Prompt-based Pre-training for Unified Question Answering
- [ ] research: wandb, sweep, biLSTM layer, backbone, scheduler, 7조 병렬처리


# CS
## PS
### 645. Set Mismatch
- 1~n까지의 수 중에 어떤게 없고, 어떤게 두 개 있는지 파악하기



# PAPER
## LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models

- 각 비디오 프레임을 두 개의 토큰으로 표현
  - context token, contents token
- 긴 비디오에 대해 정확한 답변을 생성하기 위해, 
- LLM: 이미지 비디오를 포함하는 instruction 데이터 수집
- VLM: 
- LLaMA-VID: 비디오의 시간 t에서의 프레임을 사용
- Text decoder, Visual Encoder 활용해서 context attention 활용



## Generative Agents: Interactive Simulacra of Human Behavior
- gpt 기반으로 게임 속 마을 시뮬레이션
- Memory system: 캐릭터들이 기억을 채워서 다음 행동에 영향
  - 가령 파티한다고 하면, 다른 캐릭터에게 전달하거나 확인 받음
  - 메모리가 엄청 쌓이는데, 이 때 gpt에게 중요한것만 전달 (Retrieve)
  - REcency, Importance(점수부여 기준 높음), Relevance
  - 위의 기준만으론, 사람처럼 선택할 수 없음. 가령 연구에 관심 많은 캐릭터가 연구 좋아하는 캐릭터보다 기숙사 룸메이트 선택
  - Reflection이라는 다른 방법 적용
- Reflection: 캐릭터의 관심사 등 추출해서 메모리에 반영
- Plan: 시간 정보만 가지고, 답변 생성하면 퀄 안좋음. 대략적인 일상 계획
- 성능 평가 구체적으로 어떻게?


## Osprey: Pixel Understanding with Visual Instruction Tuning


- 최근 MLLms은 통의 이미지는 잘 처리못함
- 공간에서 bounding box 뿐만 아니라 정확한 region 지정으로 성능 높이고자 함
- Imgae에 대한 mask된 것까지 처리


