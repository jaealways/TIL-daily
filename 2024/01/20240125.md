# TOC
- [PAPER](#paper)
  * [Robust Speech Recognition via Large-Scale Weak Supervision](#robust-speech-recognition-via-large-scale-weak-supervision)
  * [Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations](#knowledge-plugins--enhancing-large-language-models-for-domain-specific-recommendations)


# Goal

- [x] PS: 1143. Longest Common Subsequence
- [x] PAPER: OpenAssistant Conversations - Democratizing Large Language Model Alignment	
- [ ] PAPER: tldr, CV listc
- [ ] LEC: C basics


# PAPER 
## Robust Speech Recognition via Large-Scale Weak Supervision
- Whisper 논문
- zero-shot 환경에서도 작동하는 음성인식 모델
- 기존 음성인식 모델 Wave2Vec: 라벨링 없는 데이터로 음성인식기 만들 수 있음
- High quality supervised dataset으로 제한이 있을 수 밖에 없음
- 오디오에선 노이즈 도움 됨, 반면 텍스트에선 도움 안됨
- ASR: 자동음성인식, ARS로 생성된 많은 데이터 제거
- 기계가 작성한 데이터?
  - 구두점이 단순. 전체 대문자 혹은 소문자 등
- 오디오와 언어 텍스트가 일치할 때만
- 모델 구조는 트랜스포머와 유사하게 사용
- 모든 오디오는 log-mel 스펙트럼으로 변환
  - log-mel: 음성 이미지로 변환해서 푸리에 변환?
- 텍스트 역정규화: 텍스트를 원래의 형태로 되돌리는 과정
- Whisper 사용해서 오류율 크게 감소
- 영어가 아닌 다른 언어 늘리면 성능 개선 될 듯


## Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations

- LLM을 도메인 특화 지식에 결합
- 특정 관련 도메인 지식
  - 규모가 크고 지속적으로 진화되는 데이터
  - 데이터에 반영된 특정 작업 패턴
- DOKE: domain specific knowledge
  - task에 효과적인 지식준비 단계
  - 지식을 customize하는 단계

